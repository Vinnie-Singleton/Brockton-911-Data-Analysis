{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generates a text file containing all of the pdf urls that will be downloaded\n",
    "def getPDFURLs(endingPageNumber,outputPath):\n",
    "    # add headers to prevent 502 request error\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Mobile Safari/537.36'}\n",
    "\n",
    "    f = open(outputPath,'w')\n",
    "    f.close()\n",
    "\n",
    "    baseUrl = \"http://www.brocktonpolice.com/category/police-log/page/\"\n",
    "\n",
    "    pageNumber = endingPageNumber\n",
    "    # check if file exists and create it if it does not\n",
    "    if not os.path.isfile(outputPath):\n",
    "        f.open(outputPath,'w')\n",
    "        f.close()\n",
    "\n",
    "    \n",
    "    f = open(outputPath, 'a')\n",
    "\n",
    "    while(pageNumber > 0):\n",
    "        fullPath = baseUrl+str(pageNumber)+\"/\"\n",
    "        \n",
    "        data = requests.get(fullPath, headers=headers, stream=True)    \n",
    "        soup = bs(data.content, 'html.parser')\n",
    "        basePageLinks = []\n",
    "        # Get all links on the first page\n",
    "        for e in soup.find_all('h2'):\n",
    "            for link in e.find_all('a', href=True):\n",
    "                basePageLinks.append(link['href'])\n",
    "        # Follow the links from the first page to the second page and record the PDF urls\n",
    "        for link in basePageLinks:\n",
    "            data = requests.get(link, headers=headers, stream=True)\n",
    "            soup = bs(data.content, 'html.parser')\n",
    "            for e in soup.find_all('article'):\n",
    "                for l in e.find_all('a', href=True):\n",
    "                    if '.pdf' in l['href']:\n",
    "                        f.write(l['href']+'\\n')\n",
    "\n",
    "        pageNumber -= 1\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads all of the pdfs from the list of links previously recorded\n",
    "def downloadPDFs(pdfLinks,outputPath):\n",
    "    # add headers to prevent 502 request error\n",
    "    headers = {\n",
    "        'user-agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Mobile Safari/537.36'}\n",
    "\n",
    "    # ChromeDriver requires backslashes otherwise the download is not automatic and a popup is generated\n",
    "    fp = outputPath.replace(\"/\",\"\\\\\")\n",
    "\n",
    "    if not os.path.exists(fp):\n",
    "        os.makedirs(fp)\n",
    "    \n",
    "    print(\"Downloading to\",fp)\n",
    "\n",
    "    # Set auto download options\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option('prefs', {\n",
    "    \"download.default_directory\": fp,\n",
    "    \"download.prompt_for_download\": False,\n",
    "    \"download.directory_upgrade\": True,\n",
    "    \"plugins.always_open_pdf_externally\": True})\n",
    "\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(),options=options)\n",
    "    \n",
    "    for link in pdfLinks:\n",
    "        driver.get(link)\n",
    "        sleep(7)\n",
    "    \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a Text File That Holds the URLs where the PDFs are Located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = \"./brocktonPDFUrls.txt\"\n",
    "\n",
    "# get the most recent n pages of data\n",
    "getPDFURLs(27,outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the PDFs Based on the Links in the Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = \"./PDFs\"\n",
    "pdfs = []\n",
    "\n",
    "f = open(\"./brocktonPDFUrls.txt\",'r')\n",
    "for line in f:\n",
    "    pdfs.append(line.strip('\\n'))\n",
    "\n",
    "f.close()\n",
    "\n",
    "downloadPDFs(pdfs,outputPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
